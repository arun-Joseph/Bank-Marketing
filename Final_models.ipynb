{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/bank_data_dummies.pkl')\n",
    "data1 = data.drop(columns=['y'], axis=1)\n",
    "\n",
    "x = np.ones([len(data1),len(list(data1))+1])\n",
    "y = np.ones([len(data1)])\n",
    "\n",
    "x[:,1:len(list(data1))+1] = data1[:]\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    y[i] = data['y'][i]\n",
    "\n",
    "x_train = x[:int(0.7*len(data))]\n",
    "y_train = y[:int(0.7*len(data))]\n",
    "x_test = x[int(0.7*len(data)):]\n",
    "y_test = y[int(0.7*len(data)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive values before sampling: 1840 \n",
      "No. of negative values before sampling: 29807 \n",
      "\n",
      "No. of positive values after sampling: 31280 \n",
      "No. of negative values after sampling: 29807 \n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "d=0\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i]==1:\n",
    "        c+=1\n",
    "    else:\n",
    "        d+=1\n",
    "        \n",
    "print('No. of positive values before sampling: {} '.format(c))\n",
    "print('No. of negative values before sampling: {} '.format(d))\n",
    "print()\n",
    "x_train_sampled = np.ones([len(x_train)+16*c,x_train.shape[1]])\n",
    "y_train_sampled = np.ones([len(y_train)+16*c])\n",
    "x_train_sampled[:len(x_train)]= x_train[:]\n",
    "y_train_sampled[:len(y_train)]= y_train[:]\n",
    "\n",
    "m1 = len(x_train)\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i]==1:\n",
    "        for j in range(16):\n",
    "            x_train_sampled[m1]=x_train[i]\n",
    "            y_train_sampled[m1]=y_train[i]\n",
    "            m1+=1\n",
    "\n",
    "c=0\n",
    "d=0\n",
    "for i in range(len(x_train_sampled)):\n",
    "    if y_train_sampled[i]==1:\n",
    "        c+=1\n",
    "    else:\n",
    "        d+=1\n",
    "\n",
    "print('No. of positive values after sampling: {} '.format(c))\n",
    "print('No. of negative values after sampling: {} '.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='poly', degree=3, gamma='scale')\n",
    "svc.fit(x_train_sampled, y_train_sampled.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(x_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(x_train)\n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=5)\n",
    "rf.fit(x_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(testing):  0.728914774402831\n",
      "Precision(testing):  0.3496042216358839\n",
      "Recall(testing):  0.07683386488837345\n",
      "F1 score(testing):  0.12598050867601618\n",
      "\n",
      "Accuracy(training):  0.9968401428255443\n",
      "Precision(training):  0.9484536082474226\n",
      "Recall(training):  1.0\n",
      "F1 score(training):  0.9735449735449735\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "predictions = y_pred.copy()\n",
    "for i in range(predictions.size):\n",
    "    if(predictions[i] >= 0.48):\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_train)\n",
    "predictions = y_pred.copy()\n",
    "for i in range(predictions.size):\n",
    "    if(predictions[i] >= 0.48):\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0\n",
    "        \n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "lr.fit(x_train_sampled, y_train_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(testing):  0.674435542607429\n",
      "Precision(testing):  0.367248322147651\n",
      "Recall(testing):  0.45088991430454844\n",
      "F1 score(testing):  0.40479360852197077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(training):  0.6514045565140456\n",
      "Precision(training):  0.09853249475890985\n",
      "Recall(training):  0.6130434782608696\n",
      "F1 score(training):  0.16977724262492472\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(x_train)\n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc = MLPClassifier(hidden_layer_sizes=(100, 200, 100), activation='relu', solver='lbfgs', \n",
    "                     alpha=0.005, learning_rate_init=0.001, shuffle=False)\n",
    "mlpc.fit(x_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlpc.predict(x_test)\n",
    "metrics.confusion_matrix(y_test, predictions)\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlpc.predict(x_train)\n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

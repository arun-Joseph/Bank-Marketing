{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/bank_data_dummies.pkl')\n",
    "data1 = data.drop(columns=['y'], axis=1)\n",
    "\n",
    "x = np.ones([len(data1),len(list(data1))+1])\n",
    "y = np.ones([len(data1)])\n",
    "\n",
    "x[:,1:len(list(data1))+1] = data1[:]\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    y[i] = data['y'][i]\n",
    "\n",
    "x_train = x[:int(0.7*len(data))]\n",
    "y_train = y[:int(0.7*len(data))]\n",
    "x_test = x[int(0.7*len(data)):]\n",
    "y_test = y[int(0.7*len(data)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive values before sampling: 1840 \n",
      "No. of negative values before sampling: 29807 \n",
      "\n",
      "No. of positive values after sampling: 31280 \n",
      "No. of negative values after sampling: 29807 \n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "d=0\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i]==1:\n",
    "        c+=1\n",
    "    else:\n",
    "        d+=1\n",
    "        \n",
    "print('No. of positive values before sampling: {} '.format(c))\n",
    "print('No. of negative values before sampling: {} '.format(d))\n",
    "print()\n",
    "x_train_sampled = np.ones([len(x_train)+16*c,x_train.shape[1]])\n",
    "y_train_sampled = np.ones([len(y_train)+16*c])\n",
    "x_train_sampled[:len(x_train)]= x_train[:]\n",
    "y_train_sampled[:len(y_train)]= y_train[:]\n",
    "\n",
    "m1 = len(x_train)\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i]==1:\n",
    "        for j in range(16):\n",
    "            x_train_sampled[m1]=x_train[i]\n",
    "            y_train_sampled[m1]=y_train[i]\n",
    "            m1+=1\n",
    "\n",
    "c=0\n",
    "d=0\n",
    "for i in range(len(x_train_sampled)):\n",
    "    if y_train_sampled[i]==1:\n",
    "        c+=1\n",
    "    else:\n",
    "        d+=1\n",
    "\n",
    "print('No. of positive values after sampling: {} '.format(c))\n",
    "print('No. of negative values after sampling: {} '.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='poly', degree=3, gamma='scale',C=1)\n",
    "svc.fit(x_train_sampled, y_train_sampled.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5835 4280]\n",
      " [1170 2279]]\n",
      "Accuracy(testing):  0.5982011206133884\n",
      "Precision(testing):  0.34746150327793873\n",
      "Recall(testing):  0.6607712380400116\n",
      "F1 score(testing):  0.4554356514788169\n"
     ]
    }
   ],
   "source": [
    "predictions = svc.predict(x_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20824  8983]\n",
      " [  543  1297]]\n",
      "Accuracy(training):  0.6989920055613487\n",
      "Precision(training):  0.12616731517509727\n",
      "Recall(training):  0.7048913043478261\n",
      "F1 score(training):  0.214026402640264\n"
     ]
    }
   ],
   "source": [
    "predictions = svc.predict(x_train)\n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=5)\n",
    "rf.fit(x_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9644  471]\n",
      " [3163  286]]\n",
      "Accuracy(testing):  0.7320849306989089\n",
      "Precision(testing):  0.37780713342140027\n",
      "Recall(testing):  0.08292258625688606\n",
      "F1 score(testing):  0.13599619591060388\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "predictions = y_pred.copy()\n",
    "for i in range(predictions.size):\n",
    "    if(predictions[i] >= 0.48):\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29715    92]\n",
      " [    0  1840]]\n",
      "Accuracy(training):  0.9970929313995007\n",
      "Precision(training):  0.9523809523809523\n",
      "Recall(training):  1.0\n",
      "F1 score(training):  0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_train)\n",
    "predictions = y_pred.copy()\n",
    "for i in range(predictions.size):\n",
    "    if(predictions[i] >= 0.48):\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0\n",
    "        \n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2,include_bias=False)\n",
    "poly.fit_transform(x_train_sampled)\n",
    "lr = linear_model.LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "lr.fit(x_train_sampled, y_train_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6551 3564]\n",
      " [1410 2039]]\n",
      "Accuracy(testing):  0.63329401356532\n",
      "Precision(testing):  0.3639121898982688\n",
      "Recall(testing):  0.5911858509712961\n",
      "F1 score(testing):  0.45050817498895274\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_test, predictions))\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19487 10320]\n",
      " [  712  1128]]\n",
      "Accuracy(training):  0.6514045565140456\n",
      "Precision(training):  0.09853249475890985\n",
      "Recall(training):  0.6130434782608696\n",
      "F1 score(training):  0.16977724262492472\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(x_train)\n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.005, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 200, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=False, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc = MLPClassifier(hidden_layer_sizes=(100, 200, 100), activation='relu', solver='lbfgs', \n",
    "                     alpha=0.005, learning_rate_init=0.001, shuffle=False)\n",
    "mlpc.fit(x_train_sampled, y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(testing):  0.6513565319964613\n",
      "Precision(testing):  0.33298538622129437\n",
      "Recall(testing):  0.3699623079153378\n",
      "F1 score(testing):  0.3505013047658289\n"
     ]
    }
   ],
   "source": [
    "predictions = mlpc.predict(x_test)\n",
    "metrics.confusion_matrix(y_test, predictions)\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1_score = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy(testing): \", accuracy)\n",
    "print(\"Precision(testing): \", precision)\n",
    "print(\"Recall(testing): \", recall)\n",
    "print(\"F1 score(testing): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26299  3508]\n",
      " [   31  1809]]\n",
      "Accuracy(training):  0.8881726545960122\n",
      "Precision(training):  0.3402294526988904\n",
      "Recall(training):  0.9831521739130434\n",
      "F1 score(training):  0.5055190722369709\n"
     ]
    }
   ],
   "source": [
    "predictions = mlpc.predict(x_train)\n",
    "print(metrics.confusion_matrix(y_train, predictions))\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "precision = metrics.precision_score(y_train, predictions)\n",
    "recall = metrics.recall_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions)\n",
    "\n",
    "print(\"Accuracy(training): \", accuracy)\n",
    "print(\"Precision(training): \", precision)\n",
    "print(\"Recall(training): \", recall)\n",
    "print(\"F1 score(training): \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
